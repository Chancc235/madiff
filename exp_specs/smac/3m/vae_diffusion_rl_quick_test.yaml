meta_data:
  exp_name: "vae_diffusion_rl_quick_test"
  script_path: "run_scripts/train_dc.py"
  num_workers: 1

variables:
  seed: [100]

constants:
  # misc
  seed: 100
  job_name: "{dataset}/h_{horizon}-hh_{history_horizon}-{model}-r_{returns_scale}-guidew_{condition_guidance_w}"
  horizon: 8  # Add horizon to constants
  returns_scale: 20  # Add returns_scale
  env_type: "smac"
  n_agents: 3
  use_action: True
  discrete_action: False
  residual_attn: True
  decentralized_execution: False
  use_zero_padding: False
  pred_future_padding: True

  # model - use DiffusionBackbone for testing
  model: "models.DiffusionBackbone"
  diffusion: "models.OfflineDiffusionRL"
  share_inv: True
  history_horizon: 2  # Reduced for faster testing
  n_diffusion_steps: 50  # Much fewer steps for testing
  action_weight: 10
  loss_weights: null
  loss_discount: 1
  dim_mults: [1, 2]  # Not used by DiffusionBackbone
  returns_condition: False  # Disable for simplicity
  predict_epsilon: True
  dim: 64  # Smaller model
  hidden_dim: 128
  condition_dropout: 0.1
  condition_guidance_w: 1.2  # Add missing parameter
  train_only_inv: False
  clip_denoised: True
  renderer: "utils.SMACRenderer"

  # VAE+Diffusion RL specific parameters - simplified
  vae_latent_dim: 32  # Smaller for testing
  trajectory_latent_dim: 64
  vae_weight: 1.0
  
  # DiffusionBackbone specific parameters - simplified
  backbone_layers: 2  # Fewer layers for testing
  backbone_hidden_dim: 128
  
  # Offline RL parameters - enabled
  conservative_weight: 0
  awac_weight: 0.5
  use_conservative_loss: true   # Enable conservative loss
  use_behavior_cloning: true    # Enable behavioral cloning
  bc_weight: 0.5

  # dataset
  loader: "datasets.VAESequenceDataset"
  normalizer: "CDFNormalizer"
  dataset: "3m-Medium"
  max_n_episodes: 50000  # Increase to allow more episodes
  preprocess_fns: []
  use_padding: True
  discount: 0.99
  max_path_length: 60  # Increase to handle longer trajectories
  termination_penalty: 0.0
  include_returns: true  # Enable returns for better reward information
  include_env_ts: true   # Enable environment timesteps

  # training - quick test parameters
  n_steps_per_epoch: 500  # Small for quick testing
  n_train_steps: 2000     # Small total steps for quick testing
  batch_size: 32         # Reasonable batch size
  learning_rate: 0.001
  gradient_accumulate_every: 1
  ema_decay: 0.99
  log_freq: 100          # Frequent logging
  save_freq: 500         # Frequent saves
  sample_freq: 0         # Disable sampling to avoid rendering errors
  n_saves: 4
  save_parallel: False
  n_reference: 1
  save_checkpoints: True # Enable checkpoints for evaluation

  # eval - enabled and frequent for testing
  evaluator: "utils.MADEvaluator"
  num_eval: 10           # Number of evaluation episodes
  num_envs: 2            # Number of parallel environments for evaluation (smaller for testing)
  eval_freq: 500         # Evaluate every 500 steps (same as save_freq)
  test_ret: 1.0          # Expected return value for evaluation (SMAC uses 1.0)

  # Offline RL Trainer parameters
  q_learning_freq: 10          # Frequency of Q-Learning updates
  target_update_freq: 1000     # Frequency of target network updates
  conservative_update_freq: 5  # Frequency of conservative updates 
  bc_update_freq: 2           # Frequency of behavior cloning updates

  # load checkpoint
  continue_training: false

  # Disable online RL for pure offline training
  online_training: false 